{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5dfe9bb",
   "metadata": {},
   "source": [
    "# Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9a18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/sal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-03 08:35:52,662\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import torch.multiprocessing as mp\n",
    "import random, numpy as np\n",
    "import time\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bb064",
   "metadata": {},
   "source": [
    "* Setting Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c01a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"dataset_name\": \"HuggingFaceH4/MATH-500\",\n",
    "    \"dataset_split\": \"test\",\n",
    "    \"prompt\": \"\"\"Solve the following math problem efficiently and clearly:\n",
    "\n",
    "- For simple problems (2 steps or fewer):\n",
    "Provide a concise solution with minimal explanation.\n",
    "\n",
    "- For complex problems (3 steps or more):\n",
    "Use this step-by-step format:\n",
    "\n",
    "## Step 1: [Concise description]\n",
    "[Brief explanation and calculations]\n",
    "\n",
    "## Step 2: [Concise description]\n",
    "[Brief explanation and calculations]\n",
    "\n",
    "...\n",
    "\n",
    "Therefore, the final answer is: $\\\\boxed{answer}$. I hope it is correct.\n",
    "\n",
    "Where [answer] is just the final number or expression that solves the problem.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6734afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed \n",
    "def set_seed(seed):\n",
    "    mp.set_sharing_strategy('file_system')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ff9db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-03 07:36:39 arg_utils.py:953] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 08-03 07:36:39 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 08-03 07:36:39 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 08-03 07:36:42 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 08-03 07:36:42 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  5.43it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:00<00:01,  1.83it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:01<00:00,  1.46it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.35it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:02<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-03 07:36:46 model_runner.py:1071] Loading model weights took 14.9888 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-03 07:36:46 gpu_executor.py:122] # GPU blocks: 28041, # CPU blocks: 2048\n",
      "INFO 08-03 07:36:46 gpu_executor.py:126] Maximum concurrency for 131072 tokens per request: 3.42x\n",
      "INFO 08-03 07:36:48 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-03 07:36:48 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-03 07:36:55 model_runner.py:1530] Graph capturing finished in 8 secs.\n"
     ]
    }
   ],
   "source": [
    "# Start llm engine \n",
    "llm = LLM(model=config[\"model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20bbb5",
   "metadata": {},
   "source": [
    "# Test trials for majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6db5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# Majority voting with normalization\n",
    "def find_majority_answer(completions: List[str]) -> Tuple[str, Dict[str, int]]:\n",
    "    canonical_groups = defaultdict(int)\n",
    "    canonical_to_original = {}\n",
    "\n",
    "    for output in completions:\n",
    "        # Step 1: Try extracting from boxed expression\n",
    "        boxed = last_boxed_only_string(output)\n",
    "        if boxed:\n",
    "            try:\n",
    "                answer = remove_boxed(boxed)\n",
    "            except Exception:\n",
    "                continue  # skip this sample if boxed parsing fails\n",
    "        else:\n",
    "            continue  # skip if no boxed answer found\n",
    "\n",
    "        # Step 2: Normalize and vote\n",
    "        canonical = strip_string(answer)\n",
    "        canonical_groups[canonical] += 1\n",
    "\n",
    "        if canonical not in canonical_to_original:\n",
    "            canonical_to_original[canonical] = answer\n",
    "\n",
    "    # Step 3: Determine majority\n",
    "    if not canonical_groups:\n",
    "        return \"\", {}  # No valid votes\n",
    "\n",
    "    max_count = max(canonical_groups.values())\n",
    "    for canonical, count in canonical_groups.items():\n",
    "        if count == max_count:\n",
    "            return canonical_to_original[canonical], dict(canonical_groups)\n",
    "\n",
    "# Inference wrapper with prompt formatting\n",
    "def majority_vote(batch: Dict, config: Dict, sampling_params, llm) -> Dict:\n",
    "    prompt = batch[\"problem\"]\n",
    "    full_prompt = f\"{config['prompt'].strip()}\\n\\n### Problem:\\n{prompt.strip()}\"\n",
    "\n",
    "    # generate가 List[RequestOutput] 반환\n",
    "    request_outputs = llm.generate(full_prompt, sampling_params=sampling_params)\n",
    "\n",
    "    # 각 RequestOutput 객체에서 outputs 리스트를 펼쳐서 CompletionOutput.text 수집\n",
    "    completions = []\n",
    "    for request_output in request_outputs:\n",
    "        completions.extend([output.text for output in request_output.outputs])\n",
    "\n",
    "    pred, votes = find_majority_answer(completions)\n",
    "\n",
    "    return {\n",
    "        \"completions\": completions,\n",
    "        \"pred\": pred,\n",
    "        \"votes\": votes,\n",
    "    }\n",
    "\n",
    "# Prediction vs GT comparison\n",
    "def evaluate_single(predicted: str, gt_solution: str) -> bool:\n",
    "    pred_norm = strip_string(predicted)\n",
    "    try:\n",
    "        gt_extracted = remove_boxed(last_boxed_only_string(gt_solution)) or gt_solution\n",
    "        gt_norm = strip_string(gt_extracted)\n",
    "    except Exception:\n",
    "        gt_norm = strip_string(gt_solution)\n",
    "    return pred_norm == gt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3d860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id'],\n",
       "    num_rows: 2\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "dataset = load_dataset(config['dataset_name'], split=config['dataset_split'])\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76e49d",
   "metadata": {},
   "source": [
    "# Power monitoring & save module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd016607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/root/search-and-learn/scripts/results')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Base directory to store all results\n",
    "base_dir = Path(\"results\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Helper function to save result and CSV placeholder\n",
    "def save_results_and_csv(problem_id: int, n: int, result: dict, is_correct: bool):\n",
    "    # Define directory\n",
    "    folder_name = f\"problem_{problem_id}_batch_{n}\"\n",
    "    folder_path = base_dir / folder_name\n",
    "    folder_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Save majority_vote result + correctness\n",
    "    result_to_save = {\n",
    "        \"pred\": result[\"pred\"],\n",
    "        \"votes\": result[\"votes\"],\n",
    "        \"completions\": result[\"completions\"],\n",
    "        \"is_correct\": is_correct\n",
    "    }\n",
    "    with open(folder_path / \"result.json\", \"w\") as f:\n",
    "        json.dump(result_to_save, f, indent=2)\n",
    "\n",
    "    # Create placeholder CSV for power monitoring\n",
    "    csv_path = folder_path / \"power_log.csv\"\n",
    "    if not csv_path.exists():\n",
    "        df = pd.DataFrame(columns=[\"timestamp\", \"power_W\"])\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Show a sample path\n",
    "base_dir.absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6874db7",
   "metadata": {},
   "source": [
    "# Main experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0da39",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'n_values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 전체 루프\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m500\u001b[39m):\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m n_values:\n\u001b[32m      4\u001b[39m         \u001b[38;5;66;03m# 1. 폴더 생성\u001b[39;00m\n\u001b[32m      5\u001b[39m         run_dir = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresults/problem_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_batch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      6\u001b[39m         os.makedirs(run_dir, exist_ok=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[31mNameError\u001b[39m: name 'n_values' is not defined"
     ]
    }
   ],
   "source": [
    "from monitor_power import monitor_npu_power_usage, monitor_gpu_power_usage, calculate_avg_power_usage\n",
    "from multiprocessing import Process, Queue\n",
    "from datetime import datetime\n",
    "\n",
    "n_values = [1, 2, 4, 8, 16, 32, 48, 64, 80, 96, 112, 128]\n",
    "\n",
    "# 전체 루프\n",
    "for idx in range(500):\n",
    "    for n in n_values:\n",
    "        # 1. 폴더 생성\n",
    "        run_dir = f\"results/problem_{idx}_batch_{n}\"\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        power_monitor_path = os.path.join(run_dir, \"device_status.csv\")\n",
    "\n",
    "        # 2. Power monitor start\n",
    "        stop_queue = Queue()\n",
    "        monitor_process = Process(\n",
    "            target=monitor_gpu_power_usage,  # or monitor_npu_power_usage\n",
    "            args=(power_monitor_path, stop_queue)\n",
    "        )\n",
    "        monitor_process.start()\n",
    "\n",
    "        # 3. Run inference\n",
    "        sampling_params = SamplingParams(\n",
    "            n=n,\n",
    "            temperature=0.8,\n",
    "            top_p=1.0,\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = majority_vote(dataset[idx], config, sampling_params, llm)\n",
    "            is_correct = is_equiv(result[\"pred\"], dataset[idx]['answer'], verbose=False)\n",
    "        except Exception as e:\n",
    "            result = {\"error\": str(e)}\n",
    "            is_correct = False\n",
    "\n",
    "        # 4. Stop power monitor\n",
    "        stop_queue.put(\"stop\")\n",
    "        monitor_process.join()\n",
    "\n",
    "        # 5. 평균 전력 사용량 계산\n",
    "        avg_power = calculate_avg_power_usage(power_monitor_path)\n",
    "\n",
    "        # 6. 결과 저장\n",
    "        output_path = os.path.join(run_dir, \"result.json\")\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"index\": idx,\n",
    "                \"batch_size\": n,\n",
    "                \"prediction\": result.get(\"pred\"),\n",
    "                \"votes\": result.get(\"votes\"),\n",
    "                \"is_correct\": is_correct,\n",
    "                \"avg_power_usage\": avg_power,\n",
    "                \"error\": result.get(\"error\", None)\n",
    "            }, f, indent=2)\n",
    "\n",
    "        print(f\"✅ Finished problem_{idx}, batch_{n}, correct={is_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68ba77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "509f39f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
