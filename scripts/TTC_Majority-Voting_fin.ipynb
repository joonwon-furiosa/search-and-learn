{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5dfe9bb",
   "metadata": {},
   "source": [
    "# Majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbe9a18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda/envs/sal/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-08-03 17:11:47,606\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "/root/miniconda/envs/sal/lib/python3.11/site-packages/vllm/connections.py:8: RuntimeWarning: Failed to read commit hash:\n",
      "No module named 'vllm._version'\n",
      "  from vllm.version import __version__ as VLLM_VERSION\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from vllm import LLM, SamplingParams\n",
    "from datasets import load_dataset\n",
    "from collections import defaultdict\n",
    "import torch.multiprocessing as mp\n",
    "import random, numpy as np\n",
    "import time\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651bb064",
   "metadata": {},
   "source": [
    "* Setting Configurations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c01a367",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_path\": \"meta-llama/Llama-3.1-8B-Instruct\",\n",
    "    \"dataset_name\": \"HuggingFaceH4/MATH-500\",\n",
    "    \"dataset_split\": \"test\",\n",
    "    \"prompt\": \"\"\"Solve the following math problem efficiently and clearly:\n",
    "\n",
    "- For simple problems (2 steps or fewer):\n",
    "Provide a concise solution with minimal explanation.\n",
    "\n",
    "- For complex problems (3 steps or more):\n",
    "Use this step-by-step format:\n",
    "\n",
    "## Step 1: [Concise description]\n",
    "[Brief explanation and calculations]\n",
    "\n",
    "## Step 2: [Concise description]\n",
    "[Brief explanation and calculations]\n",
    "\n",
    "...\n",
    "\n",
    "Therefore, the final answer is: $\\\\boxed{answer}$. I hope it is correct.\n",
    "\n",
    "Where [answer] is just the final number or expression that solves the problem.\"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6734afe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed \n",
    "def set_seed(seed):\n",
    "    mp.set_sharing_strategy('file_system')\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ff9db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 08-03 08:37:38 arg_utils.py:953] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 08-03 08:37:38 config.py:1005] Chunked prefill is enabled with max_num_batched_tokens=512.\n",
      "INFO 08-03 08:37:38 llm_engine.py:237] Initializing an LLM engine (vdev) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=meta-llama/Llama-3.1-8B-Instruct, use_v2_block_manager=True, num_scheduler_steps=1, chunked_prefill_enabled=True multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "INFO 08-03 08:37:41 model_runner.py:1060] Starting to load model meta-llama/Llama-3.1-8B-Instruct...\n",
      "INFO 08-03 08:37:42 weight_utils.py:243] Using model weights format ['*.safetensors']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:00<00:00,  3.29it/s]\n",
      "Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:01<00:01,  1.47it/s]\n",
      "Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:02<00:00,  1.25it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.23it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:03<00:00,  1.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-03 08:37:45 model_runner.py:1071] Loading model weights took 14.9888 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-03 08:37:46 gpu_executor.py:122] # GPU blocks: 28041, # CPU blocks: 2048\n",
      "INFO 08-03 08:37:46 gpu_executor.py:126] Maximum concurrency for 131072 tokens per request: 3.42x\n",
      "INFO 08-03 08:37:47 model_runner.py:1402] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 08-03 08:37:47 model_runner.py:1406] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-03 08:37:55 model_runner.py:1530] Graph capturing finished in 8 secs.\n"
     ]
    }
   ],
   "source": [
    "# Start llm engine \n",
    "llm = LLM(model=config[\"model_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de20bbb5",
   "metadata": {},
   "source": [
    "# Test trials for majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cd38ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb49cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()  # Jupyter 환경에서 필수\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# Config 설정\n",
    "api_url = \"http://localhost:8000/v1/chat/completions\"\n",
    "model_name = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "context_len = 1500\n",
    "\n",
    "# 개별 요청 함수\n",
    "async def send_prompt(session: aiohttp.ClientSession, prompt: str, idx: int, context_len: int) -> str:\n",
    "    payload = {\n",
    "        \"model\": model_name,\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_p\": 1.0,\n",
    "        \"min_tokens\": context_len-1,\n",
    "        \"max_tokens\": context_len\n",
    "    }\n",
    "    try:\n",
    "        async with session.post(api_url, json=payload) as resp:\n",
    "            data = await resp.json()\n",
    "            return data[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "    except Exception as e:\n",
    "        print(f\"[{idx}] request failed: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "# 병렬 요청 함수\n",
    "async def generate_n_responses(prompt: str, n: int) -> List[str]:\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [send_prompt(session, prompt, i, context_len) for i in range(n)]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bedb686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] The capital of France is Paris. It is the country's largest city and is known for its famous landmarks such as the Eiffel Tower, Notre Dame Cathedral, and the Louvre Museum. It is also a major cultural and economic hub, and is home to many of France's government institutions, universities, and cultural attractions. Paris has been the capital of France since 987. It is one of the most visited cities in the world and is known for its romantic atmosphere, fashion, art, and cuisine. It has also been the hub of many historical events and revolutions, and is home to many significant museums, galleries, and libraries. \n",
      "\n",
      "If you are looking for more information, I would be happy to help. Let me know what you would like to know. \n",
      "\n",
      "Is there something specific you would like to know? Would you like to know more about Paris in general, or perhaps one of the cities attractions? Let me know if you would like more information. \n",
      "\n",
      "I can also help you with more questions if you would like. Do you have any other questions about Paris or France? Please ask me. \n",
      "\n",
      "I will be happy to help you with more information. \n",
      "\n",
      "Do you have any other questions? \n",
      "\n",
      "Please let me know. If you have any questions or need further assistance, don't hesitate to ask. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Would you like more information on Paris? \n",
      "If you would like to know more about Paris, I can give you some information on what you can do there, what the city is like and some other information. \n",
      "\n",
      "What would you like to know about Paris? \n",
      "\n",
      "Maybe you would like to know more about the city's history, or perhaps you would like some information on what to do in the city? \n",
      "\n",
      "Let me know if you have any other questions about Paris. \n",
      "\n",
      "I'll be happy to help. \n",
      "\n",
      "Do you need more information about the city? \n",
      "\n",
      "If you would like to know more about the city, I can tell you more about Paris' famous landmarks, museums, and other attractions. \n",
      "\n",
      "Do you have any other questions about Paris or France? \n",
      "\n",
      "Please let me know. If you have any questions or need further assistance, don't hesitate to ask. \n",
      "\n",
      "Would you like to know more about Paris, the capital of France? \n",
      "\n",
      "If you would like to know more about Paris, I can give you some information on the city's famous landmarks, museums, and other attractions. \n",
      "\n",
      "Do you have any other questions about Paris or France? \n",
      "\n",
      "Please let me know. If you have any questions or need further assistance, don't hesitate to ask. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "What would you like to know about Paris? \n",
      "\n",
      "Maybe you would like to know more about the city's history, or perhaps you would like some information on what to do in the city? \n",
      "\n",
      "Let me know if you have any other questions about Paris. \n",
      "\n",
      "Do you need more information about the city? \n",
      "\n",
      "If you would like to know more about the city, I can tell you more about Paris' famous landmarks, museums, and other attractions. \n",
      "\n",
      "Do you have any other questions about Paris or France? \n",
      "\n",
      "If you have any questions about Paris or France, I would be happy to help. \n",
      "\n",
      "Is there something specific you would like to know? \n",
      "\n",
      "Do you want to know more about Paris' famous landmarks, museums, and other attractions? \n",
      "\n",
      "Let me know what you would like to know. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "What would you like to know about Paris? \n",
      "\n",
      "Maybe you would like to know more about the city's history, or perhaps you would like some information on what to do in the city? \n",
      "\n",
      "Let me know if you have any other questions about Paris. \n",
      "\n",
      "I'll be happy to help. \n",
      "\n",
      "Do you need more information about the city? \n",
      "\n",
      "If you would like to know more about the city, I can tell you more about Paris' famous landmarks, museums, and other attractions. \n",
      "\n",
      "Do you have any other questions about Paris or France? \n",
      "\n",
      "Let me know if you have any questions about Paris or France. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Is there something specific you would like to know? \n",
      "\n",
      "Do you want to know more about Paris' famous landmarks, museums, and other attractions? \n",
      "\n",
      "Let me know what you would like to know. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Do you want to know more about one of the city's famous landmarks? \n",
      "\n",
      "If you would like to know more about one of the city's famous landmarks, I can give you some information on the Eiffel Tower, Notre Dame Cathedral, the Louvre Museum, and other attractions. \n",
      "\n",
      "Let me know what you would like to know about one of the city's famous landmarks. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Do you have any other questions about Paris or France? \n",
      "\n",
      "If you have any questions about Paris or France, I would be happy to help. \n",
      "\n",
      "Is there something specific you would like to know? \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Do you want to know more about Paris' famous landmarks, museums, and other attractions? \n",
      "\n",
      "Let me know what you would like to know. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Do you want to know more about one of the city's famous museums? \n",
      "\n",
      "If you would like to know more about one of the city's famous museums, I can give you some information on the Louvre Museum, the Musée d'Orsay, the Musée Rodin, and other museums. \n",
      "\n",
      "Let me know what you would like to know about one of the city's famous museums. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Do you have any other questions about Paris or France? \n",
      "\n",
      "If you have any questions about Paris or France, I would be happy to help. \n",
      "\n",
      "Is there something specific you would like to know? \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Do you want to know more about the city's cuisine? \n",
      "\n",
      "If you would like to know more about the city's cuisine, I can give you some information on French cuisine, including popular dishes, restaurants, and food markets. \n",
      "\n",
      "Let me know what you would like to know about French cuisine. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Do you have any other questions about Paris or France? \n",
      "\n",
      "If you have any questions about Paris or France, I would be happy to help. \n",
      "\n",
      "Is there something specific you would like to know? \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Do you want to know more about the city's history? \n",
      "\n",
      "If you would like to know more about the city's history, I can give you some information on Paris' history, including its medieval period, its role in the French Revolution, and its impact on world history. \n",
      "\n",
      "Let me know what you would like to know about Paris' history. \n",
      "\n",
      "I'll be happy to help you with more information. \n",
      "\n",
      "Do you have any other questions about Paris or France? \n",
      "\n",
      "If you have any questions about Paris or France, I would be happy to help. \n",
      "\n",
      "Is there something specific you would like to know? \n",
      "\n",
      "I'll be happy to help you\n",
      "[1] The capital of France is Paris. Paris is known for its iconic landmarks such as the Eiffel Tower, Notre Dame Cathedral, and the Louvre Museum, among many other things. It's a popular tourist destination and a major hub for culture, fashion, and cuisine. \n",
      "\n",
      "Did you know that the name \"Paris\" is derived from the Celtic word \"Lutetia,\" and the city has been the capital of France since 987? It's a city with a rich history and a plethora of exciting experiences to explore. \n",
      "\n",
      "Is there anything specific you'd like to know about Paris or France? I'm here to help. \n",
      "\n",
      "By the way, have you been to Paris or France before? I'd love to hear about your experiences or what's on your bucket list. Let's chat about it! \n",
      "\n",
      "Let me know if I can help you with anything else. Is there another question you'd like to ask? I'm all ears! \n",
      "\n",
      "Is there anything else you'd like to know about Paris or France? I'm here to provide you with helpful information and answer any questions you may have. Feel free to ask me anything! \n",
      "\n",
      "Lastly, I want to let you know that there's a fascinating history behind the Eiffel Tower. Did you know that it was originally intended to be a temporary structure? It was built for the World's Fair in 1889 and was meant to be dismantled after the event. However, it became an instant icon of Paris and was left standing. Today, it's one of the most recognizable landmarks in the world and a symbol of French culture and engineering expertise. Isn't that cool? \n",
      "\n",
      "Let me know if there's anything else you'd like to know or if you have any other questions. I'm here to help! \n",
      "\n",
      "By the way, if you're planning a trip to Paris, I can provide you with some helpful tips and recommendations. Just let me know what you're interested in, and I'll do my best to assist you.\n",
      "\n",
      "If you're interested in learning more about Paris or France, there are many fascinating topics to explore. We could discuss the history of the city, its cultural significance, or its culinary delights. Let me know what sparks your curiosity, and I'll do my best to provide you with helpful information.\n",
      "\n",
      "Lastly, I want to thank you for your interest in learning about the capital of France. If you have any other questions or topics you'd like to discuss, please don't hesitate to ask. I'm here to help and provide you with helpful information. \n",
      "\n",
      "If you're interested in visiting Paris or France, I can provide you with some helpful travel tips and recommendations. Just let me know what you're interested in, and I'll do my best to assist you.\n",
      "\n",
      "Do you have a question about Paris or France that I can help you with? I'm here to provide you with helpful information and answer any questions you may have. Feel free to ask me anything! \n",
      "\n",
      "Lastly, if you're interested in learning more about the history of France or the Eiffel Tower, there are many fascinating resources available. We could discuss the history of the tower, its construction, or its cultural significance. Let me know what sparks your curiosity, and I'll do my best to provide you with helpful information.\n",
      "\n",
      "Do you have a question about the Eiffel Tower or the history of France? I'm here to provide you with helpful information and answer any questions you may have. Feel free to ask me anything! \n",
      "\n",
      "Lastly, I want to thank you for your interest in learning about the capital of France. Paris is a truly special city with a rich history, stunning architecture, and a vibrant cultural scene. If you have any other questions or topics you'd like to discuss, please don't hesitate to ask. I'm here to help and provide you with helpful information. \n",
      "\n",
      "By the way, did you know that Paris is home to many world-renowned museums and art galleries? The Louvre Museum, for example, is one of the largest and most visited museums in the world. It's home to an impressive collection of art and artifacts from around the world, including the famous Mona Lisa painting. \n",
      "\n",
      "Do you have a question about the Louvre Museum or the art scene in Paris? I'm here to provide you with helpful information and answer any questions you may have. Feel free to ask me anything! \n",
      "\n",
      "By the way, if you're interested in learning more about the history of the Eiffel Tower, I can provide you with some helpful information. The tower was built for the World's Fair in 1889 and was intended to be a temporary structure. However, it became an instant icon of Paris and was left standing. Today, it's one of the most recognizable landmarks in the world and a symbol of French culture and engineering expertise. \n",
      "\n",
      "Do you have a question about the Eiffel Tower or the history of Paris? I'm here to provide you with helpful information and answer any questions you may have. Feel free to ask me anything! \n",
      "\n",
      "Lastly, I want to let you know that there are many ways to experience Paris and its culture. From visiting the famous landmarks to exploring the city's many museums and art galleries, there's something for everyone. If you're interested in learning more about the city or planning a trip, I'd be happy to help.\n",
      "\n",
      "Is there anything else you'd like to know about Paris or France? I'm here to provide you with helpful information and answer any questions you may have. Feel free to ask me anything! \n",
      "\n",
      "By the way, did you know that the French language is an official language in several countries around the world? It's spoken in France, Quebec, Belgium, Switzerland, and several other territories. If you're interested in learning more about the French language or culture, I'd be happy to help. \n",
      "\n",
      "Do you have a question about the French language or culture? I'm here to provide you with helpful information and answer any questions you may have. Feel free to ask me anything! \n",
      "\n",
      "Lastly, I want to thank you for your interest in learning about the capital of France. If you have any other questions or topics you'd like to discuss, please don't hesitate to ask. I'm here to help and provide you with helpful information. \n",
      "\n",
      "By the way, did you know that the Eiffel Tower has a fascinating history? It was originally intended to be a temporary structure, but it became an instant icon of Paris and was left standing. Today, it's one of the most recognizable landmarks in the world and a symbol of French culture and engineering expertise. \n",
      "\n",
      "Do you have a question about the Eiffel Tower or the history of Paris? I'm here to provide you with helpful information and answer any questions you may have. Feel free to ask me anything! \n",
      "\n",
      "Lastly, I want to let you know that there are many ways to experience Paris and its culture. From visiting the famous landmarks to exploring the city's many museums and art galleries, there's something for everyone. If you're interested in learning more about the city or planning a trip, I'd be happy to help. \n",
      "\n",
      "Do you have a question about Paris or France that I can help you with? I'm here to provide you with helpful information and answer any questions you may have. Feel free to ask me anything! \n",
      "\n",
      "By the way, did you know that the Louvre Museum is one of\n",
      "[2] The capital of France is Paris. Paris is the largest city in France and is located at the heart of the Île-de-France region in the north-central part of the country. It is known for its iconic landmarks such as the Eiffel Tower, the Louvre Museum, and Notre-Dame Cathedral. Paris has been the capital of France since 987. \n",
      "\n",
      "The city's rich history and cultural significance make it one of the most popular tourist destinations in the world. Paris is also known for its fashion, cuisine, and romantic atmosphere, making it a favorite among artists, writers, and visitors alike. \n",
      "\n",
      "In addition to its cultural significance, Paris has played a major role in French politics, economy, and education. It is home to several prestigious universities, research institutions, and international organizations. The city's economy is driven by a mix of industries, including finance, tourism, and manufacturing. \n",
      "\n",
      "Overall, Paris is a unique and fascinating city that offers a blend of history, culture, and modernity, making it the perfect representation of the French spirit. \n",
      "\n",
      "It is worth noting that, while Paris has been the capital of France for centuries, the city was attacked by notorious terrorists in 2015 and 2016, which led to a large-scale and long term security overhaul. However, Paris has made significant efforts to rebuild and recover from these events and is generally considered a safe city for tourists. \n",
      "\n",
      "If you are planning a visit to Paris, I would recommend checking the latest safety and travel advisories before your trip. Additionally, it would be a good idea to familiarize yourself with local customs, traditions, and cultural nuances to have a more enjoyable and enriching experience. \n",
      "\n",
      "Overall, Paris is a must-visit destination for anyone interested in history, culture, art, and romance, and I highly recommend adding it to your travel bucket list! \n",
      "\n",
      "If you have any specific questions or concerns, feel free to ask, and I'll be happy to help! \n",
      "\n",
      "Hope this helps, and have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Is there anything else you would like to know about Paris or France? I'd be happy to assist you! \n",
      "\n",
      "Finally, here are some interesting facts about Paris and France:\n",
      "\n",
      "- The Eiffel Tower is 324 meters (1,063 feet) tall and was built for the 1889 World's Fair.\n",
      "- The Louvre Museum is home to the Mona Lisa painting, which is one of the most famous works of art in the world.\n",
      "- Notre-Dame Cathedral was built in the 12th century and is one of the most iconic landmarks in Paris.\n",
      "- The famous French dish \"escargots\" (snails in garlic butter) originated in the south of France.\n",
      "- The famous French fashion designer Coco Chanel was born in Saumur, France, in 1883.\n",
      "- The famous French artist Claude Monet was born in Paris in 1840.\n",
      "\n",
      "I hope you find these facts interesting! \n",
      "\n",
      "Is there anything else you would like to know about Paris or France? I'd be happy to help! \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "I am happy to provide you with more information on the capital of France. \n",
      "\n",
      "If you have any specific questions or concerns, feel free to ask, and I'll be happy to help! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "You can also ask me questions about other cities or countries. I'd be happy to help! \n",
      "\n",
      "Is there anything else you would like to know about Paris or France? I'd be happy to assist you! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "You can also ask me questions about other cities or countries. I'd be happy to help! \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "I hope you have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "I am happy to help you with any questions or concerns you may have! \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "If you are planning a visit to Paris, I would recommend checking the latest safety and travel advisories before your trip. \n",
      "\n",
      "Is there anything else you would like to know about Paris or France? I'd be happy to help! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "I hope you have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "You can also ask me questions about other cities or countries. I'd be happy to help! \n",
      "\n",
      "Is there anything else you would like to know about Paris or France? I'd be happy to assist you! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "I am happy to provide you with more information on the capital of France. \n",
      "\n",
      "Is there anything else you would like to know about Paris or France? I'd be happy to help! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "I hope you have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "You can also ask me questions about other cities or countries. I'd be happy to help! \n",
      "\n",
      "Is there anything else you would like to know about Paris or France? I'd be happy to assist you! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "I am happy to help you with any questions or concerns you may have! \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "If you are planning a visit to Paris, I would recommend checking the latest safety and travel advisories before your trip. \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "You can also ask me questions about other cities or countries. I'd be happy to help! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "You can also ask me questions about other cities or countries. I'd be happy to help! \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "If you are planning a visit to Paris, I would recommend checking the latest safety and travel advisories before your trip. \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "You can also ask me questions about other cities or countries. I'd be happy to help! \n",
      "\n",
      "Have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Is there anything else you would like to know about Paris or France? I'd be happy to help! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "I hope you have a great day! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "You can also ask me questions about other cities or countries. I'd be happy to help! \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "Best regards,\n",
      "\n",
      "[Your AI friend] \n",
      "\n",
      "You can also ask me questions about other cities or countries. I'd be happy to help! \n",
      "\n",
      "Is there anything else you would like to know about Paris\n",
      "[3] The capital of France is Paris. It is the country's largest city and the seat of its government. Paris is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, among others. It is also a major cultural, economic, and educational center. The city has a rich history dating back to the Roman era, and it has played a significant role in shaping French culture and identity. Today, Paris is one of the most visited cities in the world, attracting millions of tourists each year. \n",
      "\n",
      "However, it is worth noting, in the context of the European Union, Strasbourg is also an official seat of the institutions of the European Union, like the European Parliament. But in the context of France specifically; Paris remains the capital. However both cities in France have historical ties with the administration of France.  European Union’s Parliament is in Strasbourg and  The  Council of Europe, (which is not to be confused with the European Union’s Council in Luxembourg and in Brussels is also in Strasbourg.)  These institutions hold plenary sessions in Strasbourg. But albeit the capital of France is still Paris. and centre of the French government. \n",
      "\n",
      "Now that you know, I can tell you more about France. What do you want to know? Would you like to know more about its history, culture, or perhaps its cuisine? I'd be happy to share some interesting facts and anecdotes about this beautiful country. Let me know if there's anything in particular that you'd like to learn more about. \n",
      "\n",
      "Feel free to ask any other questions you may have, and I'll do my best to provide you with accurate and helpful information. What do you say? Would you like to learn something new? Let me know how I can help. I am here to help you with any question or topic you may want to explore.   Paris, is famous for the art, history and one of the most important cities in the world. and also very beautiful. Would you like to know some famous landmarks of Paris? It has many of, them. I would be happy to tell you and also tell you about the way they were created. Every place in Paris has a story to tell. And if you are wondering about other places to visit in France then I would be happy to tell you. France has  some amazing places to visit; \n",
      "\n",
      "I think I have written enough for now. If you would like to hear about some of these amazing places then I would be happy to tell you. The Eiffel Tower is one of the most famous landmarks in Paris. It was built in 1889 as the entrance arch for the World's Fair, and it was intended to be a temporary structure. But it has since become a permanent part of the Paris skyline. It is 324 meters (1,063 feet) tall and is the tallest structure in Paris. The Eiffel Tower is made of iron and was built using over 18,000 pieces of wrought iron. It took over two years to build and required the labor of over 300 workers. The Eiffel Tower is now one of the most visited attractions in the world, with over 7 million visitors per year.\n",
      "\n",
      "Would you like to know how the Eiffel Tower was built? I can tell you more about the process and the people involved in its construction. The Eiffel Tower was built by a company called Eiffel & Cie, and it was designed by Gustave Eiffel, a French engineer. The tower was built using a revolutionary new technique called the \"basket-weave\" method, which involved weaving together iron beams to create a strong and lightweight structure. The Eiffel Tower was also one of the first structures to be built using this method, and it paved the way for the development of modern architecture.\n",
      "\n",
      "I am here to help you. Would you like to know some history about the Eiffel Tower or perhaps see what other famous landmarks Paris has to offer? Let me know. I am happy to tell you more.    The construction of the Eiffel Tower began in 1887 and took approximately two years to complete. The tower was built using over 18,000 pieces of wrought iron, weighing a total of around 7,300 tons. The tower's four main pillars are anchored to the ground and support the entire structure. The Eiffel Tower's lattice-like design provides exceptional strength and stability, allowing it to withstand strong winds and other environmental factors. \n",
      "\n",
      "In addition to its engineering prowess, the Eiffel Tower has also become an iconic symbol of French culture and identity. It has been the subject of numerous works of art, literature, and music, and has been featured in numerous films and television shows. The Eiffel Tower has also become a popular spot for proposals, weddings, and other romantic milestones. Its breathtaking views of the city and romantic ambiance have made it a favorite among couples and honeymooners.\n",
      "\n",
      "Notre-Dame Cathedral, which I mentioned earlier, is another famous landmark in Paris. It is a stunning example of Gothic architecture and is one of the most famous churches in the world. The cathedral was built in the 12th century and took over 200 years to complete. It features a beautiful rose window, stunning stained glass, and intricate stone carvings. The cathedral was also the site of the coronation of Napoleon Bonaparte and has been the location of numerous royal weddings and state funerals.\n",
      "\n",
      "Would you like to know more about Notre-Dame Cathedral or perhaps see what other famous landmarks Paris has to offer? Let me know. I am happy to tell you more. \n",
      "\n",
      "Let us move on to other famous landmarks in the city of Paris. The Louvre Museum is one of the world's largest and most famous museums, attracting over 10 million visitors per year. The museum is housed in a former royal palace and features an impressive collection of art and artifacts from around the world, including the Mona Lisa, which is one of the most famous paintings in the world.\n",
      "\n",
      "The Arc de Triomphe is another famous landmark in Paris, honoring the soldiers who fought and died for France. It was built in 1806 and is the largest triumphal arch in the world. The Arc de Triomphe is situated at the center of the Place Charles de Gaulle, which is one of the busiest squares in Paris.\n",
      "\n",
      "The Champs-Élysées is a famous avenue in Paris, known for its beautiful gardens, upscale shopping, and vibrant nightlife. It is one of the most famous streets in the world and is visited by millions of tourists each year. The avenue is lined with cafes, restaurants, and hotels, and features a number of beautiful fountains and statues.\n",
      "\n",
      "The Montmartre neighborhood is a charming and artistic area of Paris, known for its narrow streets, charming cafes, and stunning views of the city. It is home to a number of famous artists, including Picasso, Dalí, and Van Gogh, and is a popular spot for street performers and live music.\n",
      "\n",
      "The Palace of Versailles is a beautiful royal palace located just outside of Paris, known for its stunning architecture, beautiful gardens, and impressive art collection. It was built in the 17th century and was the principal royal residence of France from 1682 until the French Revolution. The palace features a number of beautiful fountains, statues, and\n"
     ]
    }
   ],
   "source": [
    "# 사용 예시\n",
    "prompt = \"What is the capital of France?\"\n",
    "\n",
    "# 실행\n",
    "responses = await generate_n_responses(prompt, n=4)\n",
    "for i, r in enumerate(responses):\n",
    "    print(f\"[{i}] {r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7316842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1501\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# LLaMA 3.1 tokenizer 불러오기\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "810ea642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token count: 1501\n"
     ]
    }
   ],
   "source": [
    "# 응답 예시\n",
    "text = responses[3]\n",
    "\n",
    "# 토큰 수 계산\n",
    "token_count = len(tokenizer.encode(text))\n",
    "print(f\"Token count: {token_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e41646d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The capital of France is Paris. It is the country's largest city and is known for its famous landmarks such as the Eiffel Tower, Notre Dame Cathedral, and the Louvre Museum. It is also a major cultural and economic hub, and is home to many of France's government institutions, universities, and cultural attractions. Paris has been the capital of France since 987. It is one of the most visited cities in the world and is known for its romantic atmosphere, fashion, art, and cuisine. It has also been the hub of many historical events and revolutions, and is home to many significant museums, galleries, and libraries. \\n\\nIf you are looking for more information, I would be happy to help. Let me know what you would like to know. \\n\\nIs there something specific you would like to know? Would you like to know more about Paris in general, or perhaps one of the cities attractions? Let me know if you would like more information. \\n\\nI can also help you with more questions if you would like. Do you have any other questions about Paris or France? Please ask me. \\n\\nI will be happy to help you with more information. \\n\\nDo you have any other questions? \\n\\nPlease let me know. If you have any questions or need further assistance, don't hesitate to ask. \\n\\nI'll be happy to help you with more information. \\n\\nWould you like more information on Paris? \\nIf you would like to know more about Paris, I can give you some information on what you can do there, what the city is like and some other information. \\n\\nWhat would you like to know about Paris? \\n\\nMaybe you would like to know more about the city's history, or perhaps you would like some information on what to do in the city? \\n\\nLet me know if you have any other questions about Paris. \\n\\nI'll be happy to help. \\n\\nDo you need more information about the city? \\n\\nIf you would like to know more about the city, I can tell you more about Paris' famous landmarks, museums, and other attractions. \\n\\nDo you have any other questions about Paris or France? \\n\\nPlease let me know. If you have any questions or need further assistance, don't hesitate to ask. \\n\\nWould you like to know more about Paris, the capital of France? \\n\\nIf you would like to know more about Paris, I can give you some information on the city's famous landmarks, museums, and other attractions. \\n\\nDo you have any other questions about Paris or France? \\n\\nPlease let me know. If you have any questions or need further assistance, don't hesitate to ask. \\n\\nI'll be happy to help you with more information. \\n\\nWhat would you like to know about Paris? \\n\\nMaybe you would like to know more about the city's history, or perhaps you would like some information on what to do in the city? \\n\\nLet me know if you have any other questions about Paris. \\n\\nDo you need more information about the city? \\n\\nIf you would like to know more about the city, I can tell you more about Paris' famous landmarks, museums, and other attractions. \\n\\nDo you have any other questions about Paris or France? \\n\\nIf you have any questions about Paris or France, I would be happy to help. \\n\\nIs there something specific you would like to know? \\n\\nDo you want to know more about Paris' famous landmarks, museums, and other attractions? \\n\\nLet me know what you would like to know. \\n\\nI'll be happy to help you with more information. \\n\\nWhat would you like to know about Paris? \\n\\nMaybe you would like to know more about the city's history, or perhaps you would like some information on what to do in the city? \\n\\nLet me know if you have any other questions about Paris. \\n\\nI'll be happy to help. \\n\\nDo you need more information about the city? \\n\\nIf you would like to know more about the city, I can tell you more about Paris' famous landmarks, museums, and other attractions. \\n\\nDo you have any other questions about Paris or France? \\n\\nLet me know if you have any questions about Paris or France. \\n\\nI'll be happy to help you with more information. \\n\\nIs there something specific you would like to know? \\n\\nDo you want to know more about Paris' famous landmarks, museums, and other attractions? \\n\\nLet me know what you would like to know. \\n\\nI'll be happy to help you with more information. \\n\\nDo you want to know more about one of the city's famous landmarks? \\n\\nIf you would like to know more about one of the city's famous landmarks, I can give you some information on the Eiffel Tower, Notre Dame Cathedral, the Louvre Museum, and other attractions. \\n\\nLet me know what you would like to know about one of the city's famous landmarks. \\n\\nI'll be happy to help you with more information. \\n\\nDo you have any other questions about Paris or France? \\n\\nIf you have any questions about Paris or France, I would be happy to help. \\n\\nIs there something specific you would like to know? \\n\\nI'll be happy to help you with more information. \\n\\nDo you want to know more about Paris' famous landmarks, museums, and other attractions? \\n\\nLet me know what you would like to know. \\n\\nI'll be happy to help you with more information. \\n\\nDo you want to know more about one of the city's famous museums? \\n\\nIf you would like to know more about one of the city's famous museums, I can give you some information on the Louvre Museum, the Musée d'Orsay, the Musée Rodin, and other museums. \\n\\nLet me know what you would like to know about one of the city's famous museums. \\n\\nI'll be happy to help you with more information. \\n\\nDo you have any other questions about Paris or France? \\n\\nIf you have any questions about Paris or France, I would be happy to help. \\n\\nIs there something specific you would like to know? \\n\\nI'll be happy to help you with more information. \\n\\nDo you want to know more about the city's cuisine? \\n\\nIf you would like to know more about the city's cuisine, I can give you some information on French cuisine, including popular dishes, restaurants, and food markets. \\n\\nLet me know what you would like to know about French cuisine. \\n\\nI'll be happy to help you with more information. \\n\\nDo you have any other questions about Paris or France? \\n\\nIf you have any questions about Paris or France, I would be happy to help. \\n\\nIs there something specific you would like to know? \\n\\nI'll be happy to help you with more information. \\n\\nDo you want to know more about the city's history? \\n\\nIf you would like to know more about the city's history, I can give you some information on Paris' history, including its medieval period, its role in the French Revolution, and its impact on world history. \\n\\nLet me know what you would like to know about Paris' history. \\n\\nI'll be happy to help you with more information. \\n\\nDo you have any other questions about Paris or France? \\n\\nIf you have any questions about Paris or France, I would be happy to help. \\n\\nIs there something specific you would like to know? \\n\\nI'll be happy to help you\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6db5fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple\n",
    "from collections import defaultdict\n",
    "\n",
    "# Majority voting with normalization\n",
    "def find_majority_answer(completions: List[str]) -> Tuple[str, Dict[str, int]]:\n",
    "    canonical_groups = defaultdict(int)\n",
    "    canonical_to_original = {}\n",
    "\n",
    "    for output in completions:\n",
    "        # Step 1: Try extracting from boxed expression\n",
    "        boxed = last_boxed_only_string(output)\n",
    "        if boxed:\n",
    "            try:\n",
    "                answer = remove_boxed(boxed)\n",
    "            except Exception:\n",
    "                continue  # skip this sample if boxed parsing fails\n",
    "        else:\n",
    "            continue  # skip if no boxed answer found\n",
    "\n",
    "        # Step 2: Normalize and vote\n",
    "        canonical = strip_string(answer)\n",
    "        canonical_groups[canonical] += 1\n",
    "\n",
    "        if canonical not in canonical_to_original:\n",
    "            canonical_to_original[canonical] = answer\n",
    "\n",
    "    # Step 3: Determine majority\n",
    "    if not canonical_groups:\n",
    "        return \"\", {}  # No valid votes\n",
    "\n",
    "    max_count = max(canonical_groups.values())\n",
    "    for canonical, count in canonical_groups.items():\n",
    "        if count == max_count:\n",
    "            return canonical_to_original[canonical], dict(canonical_groups)\n",
    "\n",
    "# Inference wrapper with prompt formatting\n",
    "def majority_vote(batch: Dict, config: Dict, sampling_params, llm) -> Dict:\n",
    "    prompt = batch[\"problem\"]\n",
    "    full_prompt = f\"{config['prompt'].strip()}\\n\\n### Problem:\\n{prompt.strip()}\"\n",
    "\n",
    "    # generate가 List[RequestOutput] 반환\n",
    "    request_outputs = llm.generate(full_prompt, sampling_params=sampling_params)\n",
    "\n",
    "    # 각 RequestOutput 객체에서 outputs 리스트를 펼쳐서 CompletionOutput.text 수집\n",
    "    completions = []\n",
    "    for request_output in request_outputs:\n",
    "        completions.extend([output.text for output in request_output.outputs])\n",
    "\n",
    "    pred, votes = find_majority_answer(completions)\n",
    "\n",
    "    return {\n",
    "        \"completions\": completions,\n",
    "        \"pred\": pred,\n",
    "        \"votes\": votes,\n",
    "    }\n",
    "\n",
    "# Prediction vs GT comparison\n",
    "def evaluate_single(predicted: str, gt_solution: str) -> bool:\n",
    "    pred_norm = strip_string(predicted)\n",
    "    try:\n",
    "        gt_extracted = remove_boxed(last_boxed_only_string(gt_solution)) or gt_solution\n",
    "        gt_norm = strip_string(gt_extracted)\n",
    "    except Exception:\n",
    "        gt_norm = strip_string(gt_solution)\n",
    "    return pred_norm == gt_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65e3d860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['problem', 'solution', 'answer', 'subject', 'level', 'unique_id'],\n",
       "    num_rows: 500\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "dataset = load_dataset(config['dataset_name'], split=config['dataset_split'])\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a76e49d",
   "metadata": {},
   "source": [
    "# Power monitoring & save module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cd016607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/root/search-and-learn/scripts/results')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "\n",
    "# Base directory to store all results\n",
    "base_dir = Path(\"results\")\n",
    "base_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Helper function to save result and CSV placeholder\n",
    "def save_results_and_csv(problem_id: int, n: int, result: dict, is_correct: bool):\n",
    "    # Define directory\n",
    "    folder_name = f\"problem_{problem_id}_batch_{n}\"\n",
    "    folder_path = base_dir / folder_name\n",
    "    folder_path.mkdir(exist_ok=True)\n",
    "\n",
    "    # Save majority_vote result + correctness\n",
    "    result_to_save = {\n",
    "        \"pred\": result[\"pred\"],\n",
    "        \"votes\": result[\"votes\"],\n",
    "        \"completions\": result[\"completions\"],\n",
    "        \"is_correct\": is_correct\n",
    "    }\n",
    "    with open(folder_path / \"result.json\", \"w\") as f:\n",
    "        json.dump(result_to_save, f, indent=2)\n",
    "\n",
    "    # Create placeholder CSV for power monitoring\n",
    "    csv_path = folder_path / \"power_log.csv\"\n",
    "    if not csv_path.exists():\n",
    "        df = pd.DataFrame(columns=[\"timestamp\", \"power_W\"])\n",
    "        df.to_csv(csv_path, index=False)\n",
    "\n",
    "# Show a sample path\n",
    "base_dir.absolute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6874db7",
   "metadata": {},
   "source": [
    "# Main experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd0da39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.48s/it, est. speed input: 10.73 toks/s, output: 132.35 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished problem_0, batch_1, correct=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.27s/it, est. speed input: 10.87 toks/s, output: 268.24 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished problem_0, batch_2, correct=False\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:15<00:00, 15.47s/it, est. speed input: 10.73 toks/s, output: 198.28 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished problem_0, batch_4, correct=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.12s/it, est. speed input: 10.30 toks/s, output: 586.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished problem_0, batch_8, correct=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:16<00:00, 16.98s/it, est. speed input: 9.78 toks/s, output: 1311.48 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished problem_0, batch_16, correct=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:19<00:00, 19.80s/it, est. speed input: 8.38 toks/s, output: 2438.42 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished problem_0, batch_32, correct=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:20<00:00, 20.29s/it, est. speed input: 8.18 toks/s, output: 3042.27 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished problem_0, batch_48, correct=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:24<00:00, 24.34s/it, est. speed input: 6.82 toks/s, output: 3930.99 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished problem_0, batch_64, correct=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts: 100%|██████████| 1/1 [00:25<00:00, 25.56s/it, est. speed input: 6.49 toks/s, output: 4660.41 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Finished problem_0, batch_80, correct=True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monitoring stopped.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     24\u001b[39m sampling_params = SamplingParams(\n\u001b[32m     25\u001b[39m     n=n,\n\u001b[32m     26\u001b[39m     temperature=\u001b[32m0.8\u001b[39m,\n\u001b[32m     27\u001b[39m     top_p=\u001b[32m1.0\u001b[39m,\n\u001b[32m     28\u001b[39m     max_tokens=\u001b[32m2048\u001b[39m,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     result = majority_vote(dataset[idx], config, sampling_params, llm)\n\u001b[32m     33\u001b[39m     is_correct = is_equiv(result[\u001b[33m\"\u001b[39m\u001b[33mpred\u001b[39m\u001b[33m\"\u001b[39m], dataset[idx][\u001b[33m'\u001b[39m\u001b[33manswer\u001b[39m\u001b[33m'\u001b[39m], verbose=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 42\u001b[39m, in \u001b[36mmajority_vote\u001b[39m\u001b[34m(batch, config, sampling_params, llm)\u001b[39m\n\u001b[32m     39\u001b[39m full_prompt = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m].strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m### Problem:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mprompt.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# generate가 List[RequestOutput] 반환\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m request_outputs = llm.generate(full_prompt, sampling_params=sampling_params)\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# 각 RequestOutput 객체에서 outputs 리스트를 펼쳐서 CompletionOutput.text 수집\u001b[39;00m\n\u001b[32m     45\u001b[39m completions = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/utils.py:1063\u001b[39m, in \u001b[36mdeprecate_kwargs.<locals>.wrapper.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m   1056\u001b[39m             msg += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00madditional_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m   1058\u001b[39m         warnings.warn(\n\u001b[32m   1059\u001b[39m             \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m(msg),\n\u001b[32m   1060\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,  \u001b[38;5;66;03m# The inner function takes up one level\u001b[39;00m\n\u001b[32m   1061\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1063\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/entrypoints/llm.py:353\u001b[39m, in \u001b[36mLLM.generate\u001b[39m\u001b[34m(self, prompts, sampling_params, prompt_token_ids, use_tqdm, lora_request, prompt_adapter_request, guided_options_request, priority)\u001b[39m\n\u001b[32m    343\u001b[39m     sampling_params = SamplingParams()\n\u001b[32m    345\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_and_add_requests(\n\u001b[32m    346\u001b[39m     prompts=parsed_prompts,\n\u001b[32m    347\u001b[39m     params=sampling_params,\n\u001b[32m   (...)\u001b[39m\u001b[32m    350\u001b[39m     guided_options=guided_options_request,\n\u001b[32m    351\u001b[39m     priority=priority)\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m._run_engine(use_tqdm=use_tqdm)\n\u001b[32m    354\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m LLMEngine.validate_outputs(outputs, RequestOutput)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/entrypoints/llm.py:879\u001b[39m, in \u001b[36mLLM._run_engine\u001b[39m\u001b[34m(self, use_tqdm)\u001b[39m\n\u001b[32m    877\u001b[39m total_out_toks = \u001b[32m0\u001b[39m\n\u001b[32m    878\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.llm_engine.has_unfinished_requests():\n\u001b[32m--> \u001b[39m\u001b[32m879\u001b[39m     step_outputs = \u001b[38;5;28mself\u001b[39m.llm_engine.step()\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m step_outputs:\n\u001b[32m    881\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/engine/llm_engine.py:1386\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m allow_async_output_proc:\n\u001b[32m   1383\u001b[39m     execute_model_req.async_callback = \u001b[38;5;28mself\u001b[39m.async_callbacks[\n\u001b[32m   1384\u001b[39m         virtual_engine]\n\u001b[32m-> \u001b[39m\u001b[32m1386\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.model_executor.execute_model(\n\u001b[32m   1387\u001b[39m     execute_model_req=execute_model_req)\n\u001b[32m   1389\u001b[39m \u001b[38;5;66;03m# We need to do this here so that last step's sampled_token_ids can\u001b[39;00m\n\u001b[32m   1390\u001b[39m \u001b[38;5;66;03m# be passed to the next iteration for PP.\u001b[39;00m\n\u001b[32m   1391\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.scheduler_config.is_multi_step:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/executor/gpu_executor.py:134\u001b[39m, in \u001b[36mGPUExecutor.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_model\u001b[39m(\n\u001b[32m    132\u001b[39m     \u001b[38;5;28mself\u001b[39m, execute_model_req: ExecuteModelRequest\n\u001b[32m    133\u001b[39m ) -> Optional[List[Union[SamplerOutput, PoolerOutput]]]:\n\u001b[32m--> \u001b[39m\u001b[32m134\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.driver_worker.execute_model(execute_model_req)\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/worker/worker_base.py:327\u001b[39m, in \u001b[36mLocalOrDistributedWorkerBase.execute_model\u001b[39m\u001b[34m(self, execute_model_req)\u001b[39m\n\u001b[32m    322\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    323\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_execute_time):\n\u001b[32m    324\u001b[39m         orig_model_execute_time = intermediate_tensors.tensors.get(\n\u001b[32m    325\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mmodel_execute_time\u001b[39m\u001b[33m\"\u001b[39m, torch.tensor(\u001b[32m0\u001b[39m)).item()\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m output = \u001b[38;5;28mself\u001b[39m.model_runner.execute_model(\n\u001b[32m    328\u001b[39m     model_input=model_input,\n\u001b[32m    329\u001b[39m     kv_caches=\u001b[38;5;28mself\u001b[39m.kv_cache[worker_input.virtual_engine]\n\u001b[32m    330\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.kv_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    331\u001b[39m     intermediate_tensors=intermediate_tensors,\n\u001b[32m    332\u001b[39m     num_steps=num_steps,\n\u001b[32m    333\u001b[39m     **kwargs,\n\u001b[32m    334\u001b[39m )\n\u001b[32m    336\u001b[39m model_execute_time = time.perf_counter() - start_time\n\u001b[32m    337\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m get_pp_group().is_last_rank:\n\u001b[32m    338\u001b[39m     \u001b[38;5;66;03m# output is IntermediateTensors\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/worker/model_runner_base.py:116\u001b[39m, in \u001b[36mdump_input_when_exception.<locals>._inner.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m    117\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    118\u001b[39m         timestamp = datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mH\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/worker/model_runner.py:1708\u001b[39m, in \u001b[36mModelRunner.execute_model\u001b[39m\u001b[34m(self, model_input, kv_caches, intermediate_tensors, num_steps)\u001b[39m\n\u001b[32m   1705\u001b[39m     model_input.async_callback()\n\u001b[32m   1707\u001b[39m \u001b[38;5;66;03m# Sample the next token.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1708\u001b[39m output: SamplerOutput = \u001b[38;5;28mself\u001b[39m.model.sample(\n\u001b[32m   1709\u001b[39m     logits=logits,\n\u001b[32m   1710\u001b[39m     sampling_metadata=model_input.sampling_metadata,\n\u001b[32m   1711\u001b[39m )\n\u001b[32m   1712\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.observability_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1713\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observability_config.collect_model_forward_time\n\u001b[32m   1714\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m output \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1715\u001b[39m     model_forward_end.synchronize()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/model_executor/models/llama.py:571\u001b[39m, in \u001b[36mLlamaForCausalLM.sample\u001b[39m\u001b[34m(self, logits, sampling_metadata)\u001b[39m\n\u001b[32m    569\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, logits: torch.Tensor,\n\u001b[32m    570\u001b[39m            sampling_metadata: SamplingMetadata) -> Optional[SamplerOutput]:\n\u001b[32m--> \u001b[39m\u001b[32m571\u001b[39m     next_tokens = \u001b[38;5;28mself\u001b[39m.sampler(logits, sampling_metadata)\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m next_tokens\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:274\u001b[39m, in \u001b[36mSampler.forward\u001b[39m\u001b[34m(self, logits, sampling_metadata)\u001b[39m\n\u001b[32m    271\u001b[39m logprobs = torch.log_softmax(logits, dim=-\u001b[32m1\u001b[39m, dtype=torch.float)\n\u001b[32m    273\u001b[39m \u001b[38;5;66;03m# Sample the next tokens.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m274\u001b[39m maybe_deferred_sample_results, maybe_sampled_tokens_tensor = _sample(\n\u001b[32m    275\u001b[39m     probs,\n\u001b[32m    276\u001b[39m     logprobs,\n\u001b[32m    277\u001b[39m     sampling_metadata,\n\u001b[32m    278\u001b[39m     sampling_tensors,\n\u001b[32m    279\u001b[39m     include_gpu_probs_tensor=\u001b[38;5;28mself\u001b[39m.include_gpu_probs_tensor,\n\u001b[32m    280\u001b[39m     modify_greedy_probs=\u001b[38;5;28mself\u001b[39m._should_modify_greedy_probs_inplace,\n\u001b[32m    281\u001b[39m )\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.include_gpu_probs_tensor:\n\u001b[32m    284\u001b[39m     \u001b[38;5;66;03m# Since we will defer sampler result Pythonization,\u001b[39;00m\n\u001b[32m    285\u001b[39m     \u001b[38;5;66;03m# preserve GPU-side tensors in support of later\u001b[39;00m\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# deferred pythonization of logprobs\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m maybe_sampled_tokens_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:878\u001b[39m, in \u001b[36m_sample\u001b[39m\u001b[34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[39m\n\u001b[32m    858\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_sample\u001b[39m(\n\u001b[32m    859\u001b[39m     probs: torch.Tensor,\n\u001b[32m    860\u001b[39m     logprobs: torch.Tensor,\n\u001b[32m   (...)\u001b[39m\u001b[32m    864\u001b[39m     modify_greedy_probs: \u001b[38;5;28mbool\u001b[39m,\n\u001b[32m    865\u001b[39m ) -> SampleReturnType:\n\u001b[32m    866\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    867\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    868\u001b[39m \u001b[33;03m        probs: (num_query_tokens_in_batch, num_vocab)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    876\u001b[39m \u001b[33;03m        sampled_token_ids_tensor: A tensor of sampled token ids.\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _sample_with_torch(\n\u001b[32m    879\u001b[39m         probs,\n\u001b[32m    880\u001b[39m         logprobs,\n\u001b[32m    881\u001b[39m         sampling_metadata,\n\u001b[32m    882\u001b[39m         sampling_tensors,\n\u001b[32m    883\u001b[39m         include_gpu_probs_tensor=include_gpu_probs_tensor,\n\u001b[32m    884\u001b[39m         modify_greedy_probs=modify_greedy_probs,\n\u001b[32m    885\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:847\u001b[39m, in \u001b[36m_sample_with_torch\u001b[39m\u001b[34m(probs, logprobs, sampling_metadata, sampling_tensors, include_gpu_probs_tensor, modify_greedy_probs)\u001b[39m\n\u001b[32m    835\u001b[39m maybe_deferred_args = SampleResultArgsType(\n\u001b[32m    836\u001b[39m     sampling_metadata=sampling_metadata,\n\u001b[32m    837\u001b[39m     sample_metadata=sample_metadata,\n\u001b[32m   (...)\u001b[39m\u001b[32m    840\u001b[39m     beam_search_logprobs=beam_search_logprobs,\n\u001b[32m    841\u001b[39m     sample_results_dict=sample_results_dict)\n\u001b[32m    843\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sampling_metadata.skip_sampler_cpu_output:\n\u001b[32m    844\u001b[39m     \u001b[38;5;66;03m# GPU<->CPU sync happens here.\u001b[39;00m\n\u001b[32m    845\u001b[39m     \u001b[38;5;66;03m# This also converts the sampler output to a Python object.\u001b[39;00m\n\u001b[32m    846\u001b[39m     \u001b[38;5;66;03m# Return Pythonized sampler result & sampled token ids\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m847\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m get_pythonized_sample_results(\n\u001b[32m    848\u001b[39m         maybe_deferred_args), sampled_token_ids_tensor\n\u001b[32m    849\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    850\u001b[39m     \u001b[38;5;66;03m# Defer sampler result Pythonization; return deferred\u001b[39;00m\n\u001b[32m    851\u001b[39m     \u001b[38;5;66;03m# Pythonization args & sampled token ids\u001b[39;00m\n\u001b[32m    852\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    853\u001b[39m         maybe_deferred_args,\n\u001b[32m    854\u001b[39m         sampled_token_ids_tensor,\n\u001b[32m    855\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:713\u001b[39m, in \u001b[36mget_pythonized_sample_results\u001b[39m\u001b[34m(sample_result_args)\u001b[39m\n\u001b[32m    711\u001b[39m     sample_results = _greedy_sample(seq_groups, greedy_samples)\n\u001b[32m    712\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m sampling_type \u001b[38;5;129;01min\u001b[39;00m (SamplingType.RANDOM, SamplingType.RANDOM_SEED):\n\u001b[32m--> \u001b[39m\u001b[32m713\u001b[39m     sample_results = _random_sample(seq_groups,\n\u001b[32m    714\u001b[39m                                     multinomial_samples[sampling_type])\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m sampling_type == SamplingType.BEAM:\n\u001b[32m    716\u001b[39m     sample_results = _beam_search_sample(seq_groups,\n\u001b[32m    717\u001b[39m                                          beam_search_logprobs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniconda/envs/sal/lib/python3.11/site-packages/vllm/model_executor/layers/sampler.py:512\u001b[39m, in \u001b[36m_random_sample\u001b[39m\u001b[34m(selected_seq_groups, random_samples)\u001b[39m\n\u001b[32m    499\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Run random sampling on a given samples.\u001b[39;00m\n\u001b[32m    500\u001b[39m \n\u001b[32m    501\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    509\u001b[39m \u001b[33;03m    seq_group has do_sample=False, tuple contains ([], [])\u001b[39;00m\n\u001b[32m    510\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    511\u001b[39m \u001b[38;5;66;03m# Find the maximum n value of the prompt phase requests.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m random_samples = random_samples.cpu()\n\u001b[32m    513\u001b[39m sample_idx = \u001b[32m0\u001b[39m\n\u001b[32m    514\u001b[39m results: SampleResultType = []\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from monitor_power import monitor_npu_power_usage, monitor_gpu_power_usage, calculate_avg_power_usage\n",
    "from multiprocessing import Process, Queue\n",
    "from datetime import datetime\n",
    "\n",
    "n_values = [1, 2, 4, 8, 16, 32, 48, 64, 80, 96, 112, 128]\n",
    "\n",
    "# 전체 루프\n",
    "for n in n_values:\n",
    "    for idx in range(500):\n",
    "    \n",
    "        # 1. 폴더 생성\n",
    "        run_dir = f\"results/batch_{n}/problem_{idx}\"\n",
    "        os.makedirs(run_dir, exist_ok=True)\n",
    "        power_monitor_path = os.path.join(run_dir, \"device_status.csv\")\n",
    "\n",
    "        # 2. Power monitor start\n",
    "        stop_queue = Queue()\n",
    "        monitor_process = Process(\n",
    "            target=monitor_gpu_power_usage,  # or monitor_npu_power_usage\n",
    "            args=(power_monitor_path, stop_queue)\n",
    "        )\n",
    "        monitor_process.start()\n",
    "\n",
    "        # 3. Run inference\n",
    "        sampling_params = SamplingParams(\n",
    "            n=n,\n",
    "            temperature=0.8,\n",
    "            top_p=1.0,\n",
    "            max_tokens=2048,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            result = majority_vote(dataset[idx], config, sampling_params, llm)\n",
    "            is_correct = is_equiv(result[\"pred\"], dataset[idx]['answer'], verbose=False)\n",
    "        except Exception as e:\n",
    "            result = {\"error\": str(e)}\n",
    "            is_correct = False\n",
    "\n",
    "        # 4. Stop power monitor\n",
    "        stop_queue.put(\"stop\")\n",
    "        monitor_process.join()\n",
    "\n",
    "        # 5. 평균 전력 사용량 계산\n",
    "        avg_power = calculate_avg_power_usage(power_monitor_path)\n",
    "\n",
    "        # 6. 결과 저장\n",
    "        output_path = os.path.join(run_dir, \"result.json\")\n",
    "        with open(output_path, \"w\") as f:\n",
    "            json.dump({\n",
    "                \"index\": idx,\n",
    "                \"batch_size\": n,\n",
    "                \"prediction\": result.get(\"pred\"),\n",
    "                \"votes\": result.get(\"votes\"),\n",
    "                \"is_correct\": is_correct,\n",
    "                \"avg_power_usage\": avg_power,\n",
    "                \"error\": result.get(\"error\", None)\n",
    "            }, f, indent=2)\n",
    "\n",
    "        print(f\"✅ Finished problem_{idx}, batch_{n}, correct={is_correct}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e68ba77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "509f39f3",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
